{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76607995",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf23961",
   "metadata": {},
   "source": [
    "This notebook is used to collect the restauant data within Greater Vancouver in Zomato's website. The output are csv files consisting of restaurant information, such as name, rating, cuisine type, etc separated by each district (or city) within Greater Vancouver."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4601d7",
   "metadata": {},
   "source": [
    "# Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d72b4b",
   "metadata": {},
   "source": [
    "<a href='#1.0'><b><h3>1.0 Importing Necessary Libraries<b></a><br> \n",
    "<a href='#2.0'><b><h3>2.0 Getting URLs and Names of Each City<b></a><br>\n",
    "<a href='#3.0'><b><h3>3.0 Building Functions<b></a><br>\n",
    "<a href='#3.1'><b><h4>3.1 Page Scroller<b></a><br> \n",
    "<a href='#3.2'><b><h4>3.2 Data Collection<b></a><br>\n",
    "<a href='#3.3'><b><h4>3.3 From Every City<b></a><br>\n",
    "<a href='#3.4'><b><h4>3.4 From Particular City<b></a><br>\n",
    "<a href='#4.0'><b><h3>4.0 Executing Functions<b></a><br> \n",
    "<a href='#4.1'><b><h4>4.1 From Every City<b></a><br> \n",
    "<a href='#4.2'><b><h4>4.2 From Particular City<b></a><br> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12bda93",
   "metadata": {},
   "source": [
    "<a id='1.0'></a>\n",
    "## 1.0 Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "844947eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup # For HTML parsing\n",
    "import requests # Website connections\n",
    "from time import sleep # To prevent overwhelming the server between connections\n",
    "from collections import Counter # Keep track of our term counts\n",
    "import pandas as pd # For converting results to a dataframe and bar chart plots\n",
    "import json # For parsing json\n",
    "import random # To randomize the web scraping timing to mitigate the risk of getting blocked/banned\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "922212ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931a2e00",
   "metadata": {},
   "source": [
    "<a id='2.0'></a>\n",
    "## 2.0 Getting URLs and Names of Each City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5abb5235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the beautiful soup scraper\n",
    "url='https://www.zomato.com/vancouver'\n",
    "agent = {\"User-Agent\":\"Mozilla/5.0\"}\n",
    "scrape=requests.get(url, headers=agent)\n",
    "scraper=BeautifulSoup(scrape.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "599fe68e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding the location container that contains all the districts in Vancouver\n",
    "location_container=scraper.find_all(\"div\", {\"class\": \"sc-bke1zw-0\"})[2].find_all('div', class_='sc-bke1zw-1')\n",
    "# sanity checking to see if the number in the list is the same as the number of location containers\n",
    "len(location_container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e4ca4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the links for the locations\n",
    "location_urls=[]\n",
    "location_names=[]\n",
    "for location in location_container:\n",
    "    try:\n",
    "        location_url=location.a.get('href')\n",
    "        location_urls.append(location_url)\n",
    "    except Exception as e:\n",
    "        location_urls.append('unknown link')\n",
    "# collecting each of the location names\n",
    "    try:\n",
    "        location_name=location.find('h5').get_text().split('(')[0].strip(' ') \n",
    "        location_names.append(location_name)\n",
    "    except Exception as e:\n",
    "        location_names.append('unknown location name')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a77478f",
   "metadata": {},
   "source": [
    "<a id='3.0'></a>\n",
    "## 3.0 Building Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d5011f",
   "metadata": {},
   "source": [
    "<a id='3.1'></a>\n",
    "### 3.1 Page Scroller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40810421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the scrolling to bottom of page function\n",
    "# credits go to https://stackoverflow.com/questions/20986631/how-can-i-scroll-a-web-page-using-selenium-webdriver-in-python\n",
    "def ScrollToBottomOfPage(url, loadingtime, maximumtime):\n",
    "    driver=webdriver.Chrome(r'C:/Users/Nathan Ling/Data Science Work/Week 4 Numpy and Web Scraping/chromedriver.exe')\n",
    "    driver.get(url)\n",
    "    # Get scroll height\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    loop=0\n",
    "    #prevent infinite scrolling if page ends up bottomless\n",
    "    while loop<=int(maximumtime/loadingtime):\n",
    "        # Scroll down to bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        # Wait to load page\n",
    "        sleep(loadingtime)\n",
    "\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        print(f'New height is {new_height} while previous height was {last_height}')\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "        loop+=1\n",
    "    scraper=BeautifulSoup(driver.page_source)\n",
    "    return scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddb4f15",
   "metadata": {},
   "source": [
    "<a id='3.2'></a>\n",
    "### 3.2 Collecting Restaurant Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "017cb949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_restaurant_info(location_number, restaurant_containers):\n",
    "    \n",
    "    restaurant_names=[]\n",
    "    restaurant_cuisine_types=[]\n",
    "    restaurant_ratings=[]\n",
    "    restaurant_locations=[]\n",
    "    restaurant_prices=[]\n",
    "    addresses=[]\n",
    "    phone_numbers=[]\n",
    "    num_reviews=[]\n",
    "    num_photos=[]\n",
    "    links=[]\n",
    "    \n",
    "    # Getting all the data within the restaurant containers\n",
    "    for restaurant in restaurant_containers:\n",
    "        # get restaurant names\n",
    "        try:\n",
    "            restaurant_name=restaurant.h4.get_text()\n",
    "            restaurant_names.append(restaurant_name)\n",
    "        except:\n",
    "            restaurant_names.append('unknown')\n",
    "        # get restaurant cuisine type\n",
    "        try:\n",
    "            restaurant_cuisine_type=restaurant.find_all('p')[1].get_text()\n",
    "            restaurant_cuisine_types.append(restaurant_cuisine_type)\n",
    "        except:\n",
    "            restaurant_cuisine_types.append('unknown')\n",
    "        # get restaurant rating\n",
    "        try:\n",
    "            restaurant_rating=restaurant.find('div',class_='sc-1q7bklc-1 cILgox').get_text()\n",
    "            restaurant_ratings.append(restaurant_rating)\n",
    "        except:\n",
    "            restaurant_ratings.append('unknown')\n",
    "        # get restaurant location\n",
    "        try:\n",
    "            restaurant_location=restaurant.find_all('p')[3].get_text(strip=True).split(',')[-2].strip()\n",
    "            restaurant_locations.append(restaurant_location)\n",
    "        except:\n",
    "            restaurant_locations.append('unknown')\n",
    "        # get restaurant prices\n",
    "        try:\n",
    "            restaurant_price=restaurant.find_all('p')[2].get_text(strip=True)\n",
    "            restaurant_prices.append(restaurant_price)\n",
    "        except:\n",
    "            restaurant_prices.append('unknown')\n",
    "        # get the links of each restaurant\n",
    "        try:\n",
    "            restaurant_link=restaurant.find('a').get('href')\n",
    "            links.append('https://www.zomato.com'+restaurant_link)\n",
    "        except:\n",
    "            links.append('unknown link')\n",
    "            \n",
    "    # Getting the data within each restaurant link that could not be obtained in the restaurant container\n",
    "    for j, link in enumerate(links):\n",
    "        try:\n",
    "            agent = {\"User-Agent\":\"Mozilla/5.0\"}\n",
    "            scrape=requests.get(link, headers=agent)\n",
    "            scraper=BeautifulSoup(scrape.content)\n",
    "        except:\n",
    "            pass\n",
    "        # getting the address from each restaurant\n",
    "        try:\n",
    "            address=scraper.find('p', class_=\"sc-1hez2tp-0 clKRrC\").get_text()\n",
    "            addresses.append(address)\n",
    "        except Exception as e:\n",
    "            addresses.append('N/A')\n",
    "        # getting the phone # from each restaurant\n",
    "        try:\n",
    "            phone_number=scraper.find('p', class_=\"sc-1hez2tp-0 fanwIZ\").get_text()\n",
    "            phone_numbers.append(phone_number)\n",
    "        except Exception as e:\n",
    "            phone_numbers.append('N/A')\n",
    "        # getting the # reviews from each restaurant\n",
    "        try:\n",
    "            num_review=int(scraper.find('div',class_='sc-1q7bklc-8 kEgyiI').get_text())\n",
    "            num_reviews.append(num_review)\n",
    "        except Exception as e:\n",
    "            num_reviews.append(0)\n",
    "        # getting the # photos from each restaurant\n",
    "        #sleep(random.random()*1+1)\n",
    "        try:\n",
    "            photoscrape=requests.get(link.replace('info','photos'), headers=agent)\n",
    "            #print(link)\n",
    "            #print(link.replace('info','photos'))\n",
    "            photoscraper=BeautifulSoup(photoscrape.content)\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            num_photo=int(photoscraper.find_all('span',{'class':'sc-1kx5g6g-2 kBKJxB'})[0].find('span').get_text().split(' ')[-1].strip('()'))\n",
    "            num_photos.append(num_photo)\n",
    "        except Exception as e:\n",
    "            num_photos.append(0)\n",
    "        # Creating a sleep function to reduce risk of getting banned\n",
    "        #sleep(random.random()*2+1)\n",
    "        # to see if the loop is actually running given how slow and tedious this step is\n",
    "        if (j+1)%10==0:\n",
    "            print(f'{j+1}th restaurant collected')\n",
    "        #Getting the last restaurant in the given location\n",
    "        if j==0:\n",
    "            last_restaurant=len(restaurant_locations)-restaurant_locations[::-1].index(location_names[location_number])\n",
    "            print(f'Last relevant restaurant in {location_names[location_number]} in the search query is {last_restaurant}')\n",
    "        #based on a control f function to find out where the last restaurant with the relevant location is so that the scraper\n",
    "        # doesn't end up getting the entire page\n",
    "        if (j+1)>=last_restaurant:\n",
    "            break\n",
    "    return restaurant_locations, restaurant_names, restaurant_ratings, restaurant_prices, restaurant_cuisine_types, addresses, phone_numbers, num_reviews, num_photos, last_restaurant "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41882669",
   "metadata": {},
   "source": [
    "<a id='3.3'></a>\n",
    "### 3.3 Collecting Data From Every City\n",
    "\n",
    "Data from every restaurant for every city in the Greater Vancouver area will be collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd9ff322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_restaurant_data_in_each_city(location_urls, location_names, loadtime, maxtime):\n",
    "    \n",
    "    for location_number,url in enumerate(location_urls):\n",
    "        \n",
    "        # Load the entire page of that particular location. Skips locations where the url doesn't work\n",
    "        try:\n",
    "            webdriver.Chrome(r'C:/Users/Nathan Ling/Data Science Work/Week 4 Numpy and Web Scraping/chromedriver.exe').\\\n",
    "            get(url)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        scraper=ScrollToBottomOfPage(url, loadtime, maxtime)\n",
    "        \n",
    "        # Collect all the restaurant containers\n",
    "        restaurant_containers=scraper.find_all('div',{'class':'jumbo-tracker'})\n",
    "        print(f'Total number of returned restaurants in {location_names[location_number]} is {len(restaurant_containers)}')\n",
    "\n",
    "        # Collecting the data\n",
    "        restaurant_locations, restaurant_names, restaurant_ratings, restaurant_prices, restaurant_cuisine_types, addresses, phone_numbers, num_reviews, num_photos, last_restaurant=extract_restaurant_info(location_number, restaurant_containers)\n",
    "\n",
    "        #Transforming data to a dict form\n",
    "        DataInDictForm={'Location':restaurant_locations[:last_restaurant],'Name':restaurant_names[:last_restaurant],\n",
    "                        'Rating':restaurant_ratings[:last_restaurant],'Price Range':restaurant_prices[:last_restaurant],\n",
    "                        'Cuisine Type':restaurant_cuisine_types[:last_restaurant], 'Address':addresses, \n",
    "                        'Phone Number':phone_numbers, 'Num Reviews':num_reviews, 'Num Photos':num_photos}\n",
    "\n",
    "        # changing the data to a pandas dataframe\n",
    "        df=pd.DataFrame(DataInDictForm)\n",
    "\n",
    "        # exporting the data to a csv file\n",
    "        df.to_csv(f'raw data/{url.split(\"/\")[-1].replace(\"restaurants\",\"data\")}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d63dfd3",
   "metadata": {},
   "source": [
    "<a id='3.4'></a>\n",
    "### 3.4 Collecting Data in Given City\n",
    "\n",
    "Data from every restaurant within a particular city in the Greater Vancouver area will be collected (eg. South Surrey)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "549e127b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_restaurant_data_per_city(location, location_urls, location_names, loadtime, maxtime):\n",
    "    \n",
    "    location_standardized=re.sub('[ ,/&-]','',location.lower().replace('and','&'))\n",
    "    location_names_standardized=[re.sub('[ &-]','',location_name.lower()) for location_name in location_names]\n",
    "    # Do not return anything if the user input location does not exist\n",
    "    if location_standardized not in location_names_standardized:\n",
    "        print('The location you have entered does not exist.')\n",
    "    # Only if the user input location exists do we extract the restaurant information\n",
    "    else:\n",
    "        print('The location you have entered exists.')\n",
    "        location_number=location_names_standardized.index(location_standardized)\n",
    "        url=location_urls[location_number]\n",
    "        \n",
    "        # Load the entire page of that particular location\n",
    "        scraper=ScrollToBottomOfPage(url, loadtime, maxtime)\n",
    "        \n",
    "        # Collect all the restaurant containers\n",
    "        restaurant_containers=scraper.find_all('div',{'class':'jumbo-tracker'})\n",
    "        print(f'Total number of returned restaurants in {location_names[location_number]} is {len(restaurant_containers)}')\n",
    "\n",
    "        \n",
    "        # Collecting the data\n",
    "        restaurant_locations, restaurant_names, restaurant_ratings, restaurant_prices, restaurant_cuisine_types, addresses, phone_numbers, num_reviews, num_photos, last_restaurant=extract_restaurant_info(location_number, restaurant_containers)\n",
    "\n",
    "        #Transforming data to a dict form\n",
    "        DataInDictForm={'Location':restaurant_locations[:last_restaurant],'Name':restaurant_names[:last_restaurant],\n",
    "                        'Rating':restaurant_ratings[:last_restaurant],'Price Range':restaurant_prices[:last_restaurant],\n",
    "                        'Cuisine Type':restaurant_cuisine_types[:last_restaurant], 'Address':addresses, \n",
    "                        'Phone Number':phone_numbers, 'Num Reviews':num_reviews, 'Num Photos':num_photos}\n",
    "\n",
    "        # changing the data to a pandas dataframe\n",
    "        df=pd.DataFrame(DataInDictForm)\n",
    "\n",
    "        # exporting the data to a csv file\n",
    "        df.to_csv(f'raw data/{url.split(\"/\")[-1].replace(\"restaurants\",\"data\")}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c6957c",
   "metadata": {},
   "source": [
    "<a id='4.0'></a>\n",
    "## 4.0 Executing Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851b1bb4",
   "metadata": {},
   "source": [
    "<a id='4.1'></a>\n",
    "### 4.1 From Every City\n",
    "\n",
    "**Important Note:** This function takes a very long time as each city's restaurant data is collected in Greater Vancouver. Not recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41d896de",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadtime=1\n",
    "maxtime=600\n",
    "collect_restaurant_data_in_each_city(location_urls, location_names, loadtime, maxtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c4cd23",
   "metadata": {},
   "source": [
    "<a id='4.2'></a>\n",
    "### 4.2 For a Particular City\n",
    "\n",
    "Input a city that is part of Greater Vancouver, as shown in the print statement below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31a1c65b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant cities in Greater Vancouver are: \n",
      "\n",
      "Central Richmond, Coquitlam, Downtown, West End, Central Burnaby, Mount Pleasant, Guildford, Kitsilano, North Burnaby, Kensington, Fairview, South Burnaby, New Westminster, Renfrew-Collingwood, Newton, East Richmond, Riley Park & Little Mountain, Whalley, City of Langley, Grandview, Port Coquitlam, Maple Ridge, Yaletown, Cariboo & Lougheed, Hastings-Sunrise, Fleetwood, South Surrey, Punjabi Market, Victoria-Fraserview & Killarney, Gastown\n"
     ]
    }
   ],
   "source": [
    "print(f\"Relevant cities in Greater Vancouver are: \\n\\n{', '.join(location_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e58a5d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "port coquitlam\n",
      "The location you have entered exists.\n",
      "New height is 5078 while previous height was 3449\n",
      "New height is 6666 while previous height was 5078\n",
      "New height is 9820 while previous height was 6666\n",
      "New height is 12932 while previous height was 9820\n",
      "New height is 16107 while previous height was 12932\n",
      "New height is 16972 while previous height was 16107\n",
      "New height is 16972 while previous height was 16972\n",
      "Total number of returned restaurants in Port Coquitlam is 110\n",
      "Last relevant restaurant in Port Coquitlam in the search query is 110\n",
      "10th restaurant collected\n",
      "20th restaurant collected\n",
      "30th restaurant collected\n",
      "40th restaurant collected\n",
      "50th restaurant collected\n",
      "60th restaurant collected\n",
      "70th restaurant collected\n",
      "80th restaurant collected\n",
      "90th restaurant collected\n",
      "100th restaurant collected\n",
      "110th restaurant collected\n"
     ]
    }
   ],
   "source": [
    "loadtime=1\n",
    "maxtime=600\n",
    "location=input()\n",
    "collect_restaurant_data_per_city(location, location_urls, location_names, loadtime, maxtime)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
